/*
Auto-generated by: https://github.com/pmndrs/gltfjsx
Command: npx gltfjsx@6.2.3 public/models/64f1a714fe61576b46f27ca2.glb -o src/components/Avatar.jsx -k -r public
*/

import { useAnimations, useGLTF } from "@react-three/drei";
import { useFrame } from "@react-three/fiber";
import { useEffect, useRef, useState } from "react";

import { useControls } from "leva";
import * as THREE from "three";
import { VISEMES } from "wawa-lipsync";
import { lipsyncManager } from "../App";
import { BrowserAvatarTracking } from "../services/browserAvatarTracking";
import { useTTSStore } from "../stores/ttsStore";

let setupMode = false;

export function Avatar(props) {
  const gltf = useGLTF("/models/wawalipavatar.glb");
  const { nodes, materials, scene } = gltf;
  const [cameraPermission, setCameraPermission] = useState('prompt'); // 'prompt', 'granted', 'denied'
  
  // Debug: Log the loaded model structure
  useEffect(() => {
    console.log("=== WAWALIPAVATAR.GLB DEBUG ===");
    console.log("Full GLTF:", gltf);
    console.log("Scene:", scene);
    console.log("Nodes:", nodes);
    console.log("Materials:", materials);
    console.log("Scene children:", scene?.children);
    
    // Log all objects in the scene
    if (scene) {
      scene.traverse((child) => {
        console.log(`- ${child.name} (${child.type})`, child);
      });
    }
  }, [gltf, scene, nodes, materials]);

  // Try to load animations, but handle gracefully if they don't exist or are incompatible
  const animationsGLTF = useGLTF("/models/animations.glb");
  
  const group = useRef();
  const trackingRef = useRef(null);
  
  // Check if the loaded avatar has its own animations
  const availableAnimations = scene.animations && scene.animations.length > 0 
    ? scene.animations 
    : (animationsGLTF.animations || []);
    
  const { actions, mixer } = useAnimations(availableAnimations, group);
  const [animation, setAnimation] = useState(() => {
    if (availableAnimations.length === 0) return null;
    // Look for "idle talk" first, then "Idle", then first available animation
    return availableAnimations.find((a) => a.name === "idle talk") 
      ? "idle talk" 
      : availableAnimations.find((a) => a.name === "Idle") 
      ? "Idle" 
      : availableAnimations[0]?.name || null;
  });

  // TTS state - automatically switch to "armature.001" when speaking
  const isSpeaking = useTTSStore((state) => state.isSpeaking);
  const savedAnimationRef = useRef(null); // Store the animation to return to after speaking
  const hasStartedTalking = useRef(false); // Track if we've actually started the talking animation

  const { smoothMovements, responsiveness, selectedAnimation, transitionDuration } = useControls("Avatar", {
    smoothMovements: {
      value: false, // Changed to false for snappier default
      label: "Smooth Movements",
    },
    responsiveness: {
      value: 0.8,
      min: 0.1,
      max: 1.0,
      step: 0.1,
      label: "Animation Speed",
    },
    transitionDuration: {
      value: 1.0,
      min: 0.1,
      max: 3.0,
      step: 0.1,
      label: "Transition Smoothness (s)",
    },
    selectedAnimation: {
      value: animation || "None",
      options: availableAnimations.length > 0 
        ? availableAnimations.reduce((acc, anim) => {
            acc[anim.name] = anim.name;
            return acc;
          }, {})
        : { "None": "None" },
      label: "Animation",
    },
  });

  // Debug: Log available animations
  useEffect(() => {
    console.log("Available animations:", availableAnimations.map(a => a.name));
    console.log("Avatar scene structure:", scene);
  }, [availableAnimations, scene]);

  // Initialize browser-based eye tracking (no backend needed!)
  // Handle camera permission request
  const requestCameraPermission = async () => {
    try {
      console.log("[Avatar] Requesting camera permission...");
      setCameraPermission('prompt');
      
      // Test camera access
      const stream = await navigator.mediaDevices.getUserMedia({ 
        video: { 
          width: { ideal: 640 }, 
          height: { ideal: 480 },
          facingMode: 'user'
        } 
      });
      
      // Stop the test stream
      stream.getTracks().forEach(track => track.stop());
      
      setCameraPermission('granted');
      console.log("[Avatar] ✅ Camera permission granted");
      
      // Now initialize tracking
      initializeTracking();
      
    } catch (error) {
      console.error("[Avatar] ❌ Camera permission denied:", error);
      setCameraPermission('denied');
    }
  };

  // Initialize tracking after camera permission
  const initializeTracking = async () => {
    if (!scene || trackingRef.current) return;

    console.log("[Avatar] Initializing browser-based eye tracking...");
    try {
      const tracker = new BrowserAvatarTracking(scene, {
        enableBlinking: false, // We handle blinking separately in the component
        enableMicroMovements: true
      });
      
      // Wait for initialization to complete
      await tracker.initialize();
      trackingRef.current = tracker;
      
      console.log("[Avatar] ✅ Tracking initialized and ready!");
    } catch (error) {
      console.error("[Avatar] Failed to initialize tracking:", error);
      console.warn("[Avatar] Eye tracking unavailable - app will continue without it");
    }
  };

  useEffect(() => {
    if (!scene) return;

    // Auto-initialize on desktop, but require user interaction on mobile
    const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
    
    if (!isMobile) {
      // Desktop: try to initialize immediately
      initializeTracking();
    } else {
      // Mobile: listen for camera permission event
      const handleCameraPermissionGranted = () => {
        console.log("[Avatar] Camera permission granted, initializing tracking...");
        initializeTracking();
      };
      
      window.addEventListener('cameraPermissionGranted', handleCameraPermissionGranted);
      
      return () => {
        window.removeEventListener('cameraPermissionGranted', handleCameraPermissionGranted);
      };
    }
  }, [scene]);

  // Cleanup tracking on unmount
  useEffect(() => {
    return () => {
      if (trackingRef.current) {
        console.log("[Avatar] Cleaning up tracking...");
        if (typeof trackingRef.current.disconnect === 'function') {
          trackingRef.current.disconnect();
        }
        trackingRef.current = null;
      }
    };
  }, []);

  // Handle animation changes from the control panel
  useEffect(() => {
    if (selectedAnimation && selectedAnimation !== animation) {
      setAnimation(selectedAnimation);
    }
  }, [selectedAnimation]);
  
  // AUTO-SWITCH ANIMATION WHEN SPEAKING
  // Save current animation when TTS starts, but DON'T switch yet
  // Switch only when lipsync actually begins (in useFrame)
  useEffect(() => {
    if (isSpeaking && !savedAnimationRef.current) {
      // TTS started - save current animation but DON'T switch yet
      savedAnimationRef.current = animation;
      hasStartedTalking.current = false; // Reset flag
      console.log('[Avatar] 🎤 TTS started - saved animation:', animation, '(waiting for lipsync to begin...)');
    } else if (!isSpeaking && savedAnimationRef.current) {
      // TTS ended - switch back to saved animation
      const previousAnim = savedAnimationRef.current;
      console.log('[Avatar] ✅ TTS ended - restoring:', previousAnim);
      savedAnimationRef.current = null; // Clear first to prevent re-triggering
      hasStartedTalking.current = false; // Reset flag
      setAnimation(previousAnim);
    }
  }, [isSpeaking, animation]);
  
  // Expose calibration function to window for easy access
  useEffect(() => {
    if (trackingRef.current) {
      window.calibrateAisha = () => {
        if (trackingRef.current && typeof trackingRef.current.calibrate === 'function') {
          trackingRef.current.calibrate();
          console.log('[Avatar] ✅ Calibration applied! Aisha should now look straight ahead.');
        }
      };
      window.resetCalibration = () => {
        if (trackingRef.current && typeof trackingRef.current.resetCalibration === 'function') {
          trackingRef.current.resetCalibration();
          console.log('[Avatar] 🔄 Calibration reset to defaults.');
        }
      };
    }
    return () => {
      delete window.calibrateAisha;
      delete window.resetCalibration;
    };
  }, [trackingRef.current]);
  
  // Store the current playing action for smooth crossfades
  const currentActionRef = useRef(null);
  
  useEffect(() => {
    if (animation && actions[animation]) {
      try {
        const newAction = actions[animation];
        
        // CRITICAL: Disable morph target tracks in animation to allow lipsync to work
        // Morph targets should be controlled by lipsync, not by animation
        if (newAction && newAction._clip && newAction._clip.tracks) {
          newAction._clip.tracks.forEach(track => {
            // Disable any morph target influence tracks (these override lipsync)
            if (track.name && track.name.includes('.morphTargetInfluences[')) {
              track.enabled = false;
            }
          });
        }
        
        // Smooth crossfade between animations using Three.js built-in crossfade
        if (currentActionRef.current && currentActionRef.current !== newAction) {
          // Crossfade from current to new animation with configurable duration
          const fadeDuration = transitionDuration || 1.0;
          
          // Prepare the new action
          newAction.reset();
          newAction.setEffectiveTimeScale(1);
          newAction.setEffectiveWeight(1);
          newAction.play();
          
          // Crossfade from old to new (smooth bone-level blending)
          currentActionRef.current.crossFadeTo(newAction, fadeDuration, true);
          
          console.log(`[Avatar] 🎬 Smooth crossfade (${fadeDuration.toFixed(1)}s):`, currentActionRef.current._clip.name, '→', newAction._clip.name);
        } else if (!currentActionRef.current) {
          // First time playing - just start the animation
          newAction.reset().fadeIn(transitionDuration * 0.5 || 0.5).play();
          console.log('[Avatar] ▶️ Starting animation:', newAction._clip.name);
        }
        
        currentActionRef.current = newAction;
          
      } catch (error) {
        console.warn(`Animation ${animation} failed to play:`, error);
      }
    }
  }, [animation, actions]);

  // Cache morph target mappings for better performance
  const morphTargetCache = useRef({});
  
  const lerpMorphTarget = (target, value, speed = 0.3) => {
    scene.traverse((child) => {
      if (child.isSkinnedMesh && child.morphTargetDictionary) {
        const cacheKey = `${child.name}-${target}`;
        
        // Check cache first
        if (morphTargetCache.current[cacheKey] !== undefined) {
          const index = morphTargetCache.current[cacheKey];
          if (index !== -1 && child.morphTargetInfluences[index] !== undefined) {
            child.morphTargetInfluences[index] = THREE.MathUtils.lerp(
              child.morphTargetInfluences[index],
              value,
              speed
            );
          }
          return;
        }
        
        let targetIndex = child.morphTargetDictionary[target];
        
        if (targetIndex === undefined) {
          // Try alternatives only once and cache the result
          const alternatives = {
            'aa': ['mouthOpen', 'jawOpen', 'mouth_open', 'jaw_open'],
            'E': ['mouthSmile', 'mouth_smile', 'mouthLeft', 'mouth_left'],
            'I': ['mouthClose', 'mouth_close', 'mouthRight', 'mouth_right'],
            'O': ['mouthFunnel', 'mouth_funnel', 'mouthPucker', 'mouth_pucker'],
            'U': ['mouthPucker', 'mouth_pucker', 'mouthFunnel', 'mouth_funnel']
          };
          
          if (alternatives[target]) {
            for (const alt of alternatives[target]) {
              const altIndex = child.morphTargetDictionary[alt];
              if (altIndex !== undefined && child.morphTargetInfluences[altIndex] !== undefined) {
                targetIndex = altIndex;
                break;
              }
            }
          }
        }
        
        // Cache the result (even if -1 for not found)
        morphTargetCache.current[cacheKey] = targetIndex || -1;
        
        if (targetIndex !== undefined && child.morphTargetInfluences[targetIndex] !== undefined) {
          child.morphTargetInfluences[targetIndex] = THREE.MathUtils.lerp(
            child.morphTargetInfluences[targetIndex],
            value,
            speed
          );

          if (!setupMode) {
            try {
              set({
                [target]: value,
              });
            } catch (e) {}
          }
        }
      }
    });
  };

  const [blink, setBlink] = useState(false);
  const [winkLeft, setWinkLeft] = useState(false);
  const [winkRight, setWinkRight] = useState(false);

  useFrame(() => {
    // Eye blinking
    lerpMorphTarget("eyeBlinkLeft", blink || winkLeft ? 1 : 0, 0.5);
    lerpMorphTarget("eyeBlinkRight", blink || winkRight ? 1 : 0, 0.5);

    // LIPSYNC - exactly like it was working before
    if (setupMode) {
      return;
    }

    const viseme = lipsyncManager.viseme;
    const state = lipsyncManager.state;
    const features = lipsyncManager.features;
    
    // 🎬 REALISTIC ANIMATION SWITCH: Switch to talking animation ONLY when lipsync actually starts
    if (isSpeaking && !hasStartedTalking.current && savedAnimationRef.current) {
      // Check if lipsync has actually started (volume above threshold)
      if (features && features.volume > 0.05) {
        // Find talking animation (look for "Armature.001" in the name)
        const talkingAnimation = availableAnimations.find(a => 
          a.name.toLowerCase().includes("armature.001")
        );
        
        if (talkingAnimation) {
          hasStartedTalking.current = true; // Mark as switched
          console.log('[Avatar] 🎬 Lipsync detected! Switching to talking animation:', talkingAnimation.name);
          setAnimation(talkingAnimation.name);
        }
      }
    }
    
    // Use the responsiveness slider for animation speed - optimized for TTS
    const baseSpeed = responsiveness;
    // Much faster speeds for TTS audio sync
    const activeSpeed = smoothMovements ? (state === "vowel" ? baseSpeed * 1.2 : baseSpeed * 1.5) : baseSpeed * 2.0;
    const deactiveSpeed = smoothMovements ? (state === "vowel" ? baseSpeed * 1.0 : baseSpeed * 1.2) : baseSpeed * 1.8;
    
    lerpMorphTarget(viseme, 1, activeSpeed);

    Object.values(VISEMES).forEach((value) => {
      if (viseme === value) {
        return;
      }
      lerpMorphTarget(value, 0, deactiveSpeed);
    });
    
    // Apply face tracking AFTER animations (so it overrides animation bone rotations)
    if (trackingRef.current && typeof trackingRef.current.applyTracking === 'function') {
      trackingRef.current.applyTracking();
    }
  });

  useEffect(() => {
    let blinkTimeout;
    const nextBlink = () => {
      blinkTimeout = setTimeout(() => {
        setBlink(true);
        setTimeout(() => {
          setBlink(false);
          nextBlink();
        }, 200);
      }, THREE.MathUtils.randInt(1000, 5000));
    };
    nextBlink();
    return () => clearTimeout(blinkTimeout);
  }, []);

  // Function to safely render mesh if it exists
  const renderMesh = (nodeName, materialName, withMorphTargets = false) => {
    const node = nodes[nodeName];
    const material = materials[materialName];
    
    if (!node || !node.geometry || !material) {
      return null;
    }

    const meshProps = {
      name: nodeName,
      geometry: node.geometry,
      material: material,
      skeleton: node.skeleton,
    };

    if (withMorphTargets && node.morphTargetDictionary && node.morphTargetInfluences) {
      meshProps.morphTargetDictionary = node.morphTargetDictionary;
      meshProps.morphTargetInfluences = node.morphTargetInfluences;
    }

    return <skinnedMesh key={nodeName} {...meshProps} />;
  };

  return (
    <group {...props} dispose={null} ref={group}>
      {scene ? (
        // Just render the entire scene as-is for now
        <primitive object={scene} />
      ) : (
        // Fallback: render a simple cube to show something is there
        <mesh>
          <boxGeometry args={[1, 1, 1]} />
          <meshStandardMaterial color="red" />
        </mesh>
      )}
    </group>
  );
}

useGLTF.preload("/models/wawalipavatar.glb");
// Preload animations but don't fail if they don't exist
try {
  useGLTF.preload("/models/animations.glb");
} catch (error) {
  console.warn("Could not preload animations.glb:", error);
}
